{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction Model\n",
    "\n",
    "**Churn rate** is a business term describing the rate at which customers leave or cease paying for a product or service. It's a critical figure in many businesses, as it's often the case that acquiring new customers is a lot more costly than retaining existing ones (in some cases, five to twenty times more expensive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External requirements\n",
    "This prediction model was created using Python 2.7.12 (64-bit) through the Anaconda 4.1.1 distribution. These are the installation steps:\n",
    "\n",
    "1. Download [Anaconda](https://www.continuum.io/downloads) for the operating system and architecture you are using. *Due to the large amount of data processing required for this model, the system must have at least 5GB of RAM meaning that the architecture must be 64-bit and not 32-bit.*\n",
    "2. Run installer and complete installation.\n",
    "3. Visit your system's `PATH` and make sure there are references to `/Anaconda`, `/Anaconda/Scripts` and `/Anaconda/Library/bin`. *Any changes to the `PATH` are reflected upon logout or restart depending on the system.*\n",
    "\n",
    "All libraries except for `pydotplus` already come with Anaconda. You may install libraries through Terminal or Command Prompt issuing the command:<br/>`pip install pydotplus`.\n",
    "\n",
    "Next, we need to import some necessary libraries for our project:\n",
    "* [`numpy`](http://www.numpy.org/): Allows lists to be manipulated as arrays\n",
    "* [`pandas`](http://pandas.pydata.org/): Allows arrays to be manipulated as data frames\n",
    "* [`time`](https://docs.python.org/2/library/time.html): Allows to measure time\n",
    "* [`warnings`](https://docs.python.org/2/library/warnings.html): Allows to control the warnings of the interpreter\n",
    "* [`pydotplus`](https://pypi.python.org/pypi/pydotplus): Allows to convert graph objects to images and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import time\n",
    "import warnings\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import lifelines as ll\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines import KaplanMeierFitter\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls   \n",
    "from plotly.graph_objs import *\n",
    "from pylab import rcParams\n",
    "py.sign_in('carlosgl87', 'k5copjs82g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import some packages from the `scikit-learn`, `IPython` and `matplotlib` libraries:\n",
    "\n",
    "#### Pre-processing\n",
    "* [`preprocessing`](http://scikit-learn.org/stable/modules/preprocessing.html): Allows to transform and normalize data frames\n",
    "\n",
    "#### Cross-Validation\n",
    "* [`ShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html): Allows to perform cross-validation split of training and testing sets using random permutations iteratively.\n",
    "\n",
    "#### Classifiers\n",
    "* [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html): Allows to instance Support Vector Machine Classifiers\n",
    "* [`GaussianNB`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html): Allows to instance Gaussian Na√Øve Bayes Classifiers\n",
    "* [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html): Allows to instance Decision Tree Classifiers\n",
    "* [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html): Allows to instance Random Forest Classifiers\n",
    "* [`GradientBoostingClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html): Allows to instance Gradient Boosting Classifiers\n",
    "* [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html): Allows to instance K-Nearest Neighbors Classifiers\n",
    "\n",
    "#### Searchers\n",
    "* [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html): Allows to perform grid searches over specified parameter values for an estimator\n",
    "* [`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html): Allows to perform a randomized search on hyper paremeters\n",
    "\n",
    "#### Evaluators\n",
    "* [`f1_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html): Allows to score model results using the F1 score using the weighted average of the precision and recall\n",
    "* [`accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html): Allows to score model results using the harmonic mean between precision and recall\n",
    "* [`confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix): Allows to create a confusion matrix to compare true/false positives and true/false negatives, recall and precision\n",
    "\n",
    "#### Visualizers\n",
    "* [`export_graphviz`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html): Allows to use the Graphviz module to create graphs objects from model results\n",
    "* [`display`](https://ipython.org/ipython-doc/3/api/generated/IPython.display.html): Allows to display data frames as tables\n",
    "* [`pyplot`](http://matplotlib.org/api/pyplot_api.html): Allows to create some plot in-line with the notebook\n",
    "\n",
    "#### Other modules\n",
    "* [`StringIO`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/externals/six.py): Allows to read and write file names as strings on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Now that we have imported the necessary libraries and packages, we are ready to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning:\n",
      "\n",
      "Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'D:\\\\cgamero'\n",
    "data = pandas.read_csv(path + \"\\\\churn_data_total_sinNA_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to determine how many clients we have information on, and learn about the churn rate among these clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clients: 233227\n",
      "\n",
      "Number of features: 273\n",
      "Number of clients who churn: 104598\n",
      "Number of clients who stay: 128629\n",
      "\n",
      "Churn rate of the Clients: 44.848%\n"
     ]
    }
   ],
   "source": [
    "churn_size = data.ABANDONO_A[data.ABANDONO_A == 1].count()\n",
    "churn_rate = (float(churn_size) / float(churn_size + data.ABANDONO_A[data.ABANDONO_A == 0].count()))*100\n",
    "\n",
    "print \"Total number of clients: {}\".format(data.shape[0])\n",
    "print \"\\nNumber of features: {}\".format(data.shape[1] - 1)\n",
    "print \"Number of clients who churn: {}\".format(churn_size)\n",
    "print \"Number of clients who stay: {}\".format(data.ABANDONO_A[data.ABANDONO_A == 0].count())\n",
    "print \"\\nChurn rate of the Clients: {:.3f}%\".format(churn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Now we need to take a look at the variables (features) that we have in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'COLOR', 'MODELO_VEHICULO', 'USO_VEHICULO_INICIAL', 'USO_VEHICULO_FINAL', 'USO_VEHICULO_CAMBIO', 'LIMA_PROVINCIAS', 'NSE', 'SEGMENTO', 'CAMBIO_CARRO', 'SUMA_ASEGURADA_MEDIA', 'SUMA_ASEGURADA_MEDIANA', 'SUMA_ASEGURADA_SUMA', 'PRIMA_TOTAL', 'PRIMA_INICIAL', 'PRIMA_FINAL', 'CAMBIO_PRIMA', 'ABANDONO_NA', 'ABANDONO_A', 'ANULADO_NA', 'NO_RENOVACION_NA', 'CANAL_DISTRIBUCION', 'CORREDOR_MARSH', 'CORREDOR_CONSEJEROS_CORREDORES_SEGUROS', 'CORREDOR_CORREDORES_SEGUROS_FALABELLA', 'CORREDOR_PRIMERA_CORREDORES_SEGURO', 'CORREDOR_MARIATEGUI', 'CORREDOR_LA_UNION', 'CORREDOR_CARLOS_UGAZ', 'CORREDOR_GERENCIA_RIESGOS', 'CORREDOR_GABEL', 'CORREDOR_CONTACTO_CORREDORES', 'CORREDOR_MN_ASOC', 'CORREDOR_AMERICA_BROKERS', 'CORREDOR_FBA_CORREDORES', 'CORREDOR_OTROS', 'MONTO_SINIESTRO_MEDIA', 'MONTO_SINIESTRO_MEDIANA', 'MONTO_SINIESTRO_SUMA', 'CAUSA_SINIESTROS_ATROPELLO', 'CAUSA_SINIESTROS_CHOQUE_ESTACIONADO', 'CAUSA_SINIESTROS_CHOQUE_FUGA', 'CAUSA_SINIESTROS_CHOQUE_CIRCULANDO', 'CAUSA_SINIESTROS_DANO', 'CAUSA_SINIESTROS_ROBO', 'CAUSA_SINIESTROS_OTRO', 'CAUSA_SERVICIOS', 'CAUSA_SIN_DATOS', 'CONSECUENCIA_VEHICULO_PERDIDA_PARCIAL', 'CONSECUENCIA_VEHICULO_OTRO', 'CONSECUENCIA_PASAJERO', 'SIN_CONSECUENCIA', 'CONSECUENCIA_SIN_DATOS', 'CANAL_DISTRIBUCION_1', 'MARCA_VEHICULO', 'TIPO_PLAN', 'OK_INICIAL', 'CPP_INICIAL', 'DEFICIENTE_INICIAL', 'DUDOSO_INICIAL', 'PERDIDA_INICIAL', 'RIESGOEPSYSCTRSALUD', 'RIESGORIESGOSGENERALES', 'RIESGOSALUD', 'RIESGOVEHICULOSYSOAT', 'RIESGOVIDA', 'TIPO_VEHICULO', 'RECLAMO', 'DISCONFORMIDADES', 'RECLAMO_D', 'DISCONFORMIDADES_D', 'n_NUMERO_SINIESTROS_TOTAL', 'NUMERO_SINIESTROS_TOTAL_0', 'NUMERO_SINIESTROS_TOTAL_1', 'NUMERO_SINIESTROS_TOTAL_2', 'NUMERO_SINIESTROS_TOTAL_3 - 4', 'NUMERO_SINIESTROS_TOTAL_> 5', 'n_NUMERO_CERTIFICADOS', 'NUMERO_CERTIFICADOS_1', 'NUMERO_CERTIFICADOS_2', 'NUMERO_CERTIFICADOS_3', 'NUMERO_CERTIFICADOS_4', 'NUMERO_CERTIFICADOS_>4', 'n_NUMERO_SINIESTROS_FINAL', 'NUMERO_SINIESTROS_FINAL_0', 'NUMERO_SINIESTROS_FINAL_1', 'NUMERO_SINIESTROS_FINAL_2', 'NUMERO_SINIESTROS_FINAL_3 - 4', 'NUMERO_SINIESTROS_FINAL_> 5', 'n_NUM_RIESGOS', 'NUM_RIESGOS_0', 'NUM_RIESGOS_1', 'NUM_RIESGOS_2', 'NUM_RIESGOS_>3', 'n_NUM_PRODUCTOS', 'NUM_PRODUCTOS_0', 'NUM_PRODUCTOS_1', 'NUM_PRODUCTOS_2', 'NUM_PRODUCTOS_3', 'NUM_PRODUCTOS_>4', 'CANAL_CORREDOR', 'CANAL_DIRECTO', 'CANAL_FUERZA DE VENTAS', 'CANAL_NO TRADICIONAL', 'CANAL_nan', 'n_ANO_FABRICACION', 'ANO_FABRICACION_2000-2007', 'ANO_FABRICACION_2008', 'ANO_FABRICACION_2009', 'ANO_FABRICACION_2010', 'ANO_FABRICACION_2011', 'ANO_FABRICACION_2012', 'ANO_FABRICACION_2013', 'ANO_FABRICACION_2014', 'ANO_FABRICACION_2015', 'ANO_FABRICACION_2016', 'ANO_FABRICACION_>2000', 'ANO_FABRICACION_nan', 'COLOR_AZUL', 'COLOR_BEIGE', 'COLOR_BLANCO', 'COLOR_BRONCE', 'COLOR_CELESTE', 'COLOR_DORADO', 'COLOR_GRIS', 'COLOR_GUINDA', 'COLOR_MARRON', 'COLOR_NARANJA', 'COLOR_NEGRO', 'COLOR_OTROS', 'COLOR_PLATA', 'COLOR_ROJO', 'COLOR_VERDE', 'COLOR_nan', 'MARCA_AUDI', 'MARCA_BAJAJ', 'MARCA_BMW', 'MARCA_CHERY', 'MARCA_CHEVROLET', 'MARCA_CITROEN', 'MARCA_DAIHATSU', 'MARCA_DODGE', 'MARCA_FORD', 'MARCA_GREAT WALL', 'MARCA_HONDA', 'MARCA_HYUNDAI', 'MARCA_JAC', 'MARCA_JEEP', 'MARCA_KIA', 'MARCA_MAZDA', 'MARCA_MERCEDES', 'MARCA_MITSUBISHI', 'MARCA_NISSAN', 'MARCA_OTROS', 'MARCA_RENAULT', 'MARCA_SEAT', 'MARCA_SSANGYONG', 'MARCA_SUBARU', 'MARCA_SUZUKI', 'MARCA_TOYOTA', 'MARCA_VOLKSWAGEN', 'MARCA_VOLVO', 'MARCA_nan', 'MODELO_ACCENT', 'MODELO_CERATO', 'MODELO_COROLLA', 'MODELO_ELANTRA', 'MODELO_GOL', 'MODELO_GRAND NOMADE', 'MODELO_OTROS', 'MODELO_PICANTO', 'MODELO_RAV4', 'MODELO_RIO', 'MODELO_SAIL', 'MODELO_SANTA FE', 'MODELO_SENTRA', 'MODELO_SORENTO', 'MODELO_SPORTAGE', 'MODELO_SWIFT', 'MODELO_TIIDA', 'MODELO_TUCSON', 'MODELO_VERSA', 'MODELO_YARIS', 'MODELO_nan', 'TIPO_VEHICULO_AUTO', 'TIPO_VEHICULO_BUS', 'TIPO_VEHICULO_CAMION', 'TIPO_VEHICULO_MOTO', 'TIPO_VEHICULO_OTROS', 'TIPO_VEHICULO_PICK UP', 'TIPO_VEHICULO_RURAL', 'TIPO_VEHICULO_S/C', 'TIPO_VEHICULO_STAT. WAGON', 'TIPO_VEHICULO_VAN', 'TIPO_VEHICULO_nan', 'USO_VEHICULO_INICIAL_CARGA', 'USO_VEHICULO_INICIAL_COMERCIAL', 'USO_VEHICULO_INICIAL_OTRO', 'USO_VEHICULO_INICIAL_PARTICULAR', 'USO_VEHICULO_INICIAL_PUBLICO', 'USO_VEHICULO_INICIAL_TAXI URBANO', 'USO_VEHICULO_INICIAL_TRANSPORTE DE PERSONAL', 'USO_VEHICULO_INICIAL_VEHICULO DE ALQUILER', 'USO_VEHICULO_FINAL_CARGA', 'USO_VEHICULO_FINAL_COMERCIAL', 'USO_VEHICULO_FINAL_OTRO', 'USO_VEHICULO_FINAL_PARTICULAR', 'USO_VEHICULO_FINAL_PUBLICO', 'USO_VEHICULO_FINAL_TAXI URBANO', 'USO_VEHICULO_FINAL_TRANSPORTE DE PERSONAL', 'USO_VEHICULO_FINAL_VEHICULO DE ALQUILER', 'USO_VEHICULO_CAMBIO_OTRO', 'USO_VEHICULO_CAMBIO_OTRO - PARTICULAR', 'USO_VEHICULO_CAMBIO_PARTICULAR', 'USO_VEHICULO_CAMBIO_PARTICULAR - OTRO', 'USO_VEHICULO_CAMBIO_UN PERIODO', 'n_EDAD', 'EDAD_30-35', 'EDAD_35-40', 'EDAD_40-50', 'EDAD_50-60', 'EDAD_<30', 'EDAD_>60', 'EDAD_nan', 'LIMA_PROVINCIAS_CONO', 'LIMA_PROVINCIAS_LIMA TRADICIONAL', 'LIMA_PROVINCIAS_PROVINCIA', 'LIMA_PROVINCIAS_nan', 'NSE_A', 'NSE_A1', 'NSE_A2', 'NSE_B1', 'NSE_B2', 'NSE_C1', 'NSE_C2', 'NSE_D1', 'NSE_D2', 'NSE_E', 'NSE_nan', 'SEGMENTO_MASIVO', 'SEGMENTO_ORO 1', 'SEGMENTO_ORO 2', 'SEGMENTO_ORO 3', 'SEGMENTO_PLATINO 1', 'SEGMENTO_PLATINO 2', 'SEGMENTO_nan', 'CAMBIO_PRIMA_AUMENTO', 'CAMBIO_PRIMA_IGUAL', 'CAMBIO_PRIMA_REDUCIO', 'CAMBIO_PRIMA_UN PERIODO', 'ID_PRODUCTO', 'TIPO_PLAN_COLECTIVA', 'TIPO_PLAN_CORPORATIVA', 'TIPO_PLAN_GARANTIA', 'TIPO_PLAN_MODULAR', 'TIPO_PLAN_NO TRADICIONAL', 'TIPO_PLAN_NO USAR', 'TIPO_PLAN_PREMIER', 'TIPO_PLAN_PROVINCIA', 'TIPO_PLAN_TIMON CAMBIADO', 'TIPO_PLAN_nan', 'n_TIEMPO_FABRICACION', 'TIEMPO_FABRICACION_0', 'TIEMPO_FABRICACION_1', 'TIEMPO_FABRICACION_2', 'TIEMPO_FABRICACION_3', 'TIEMPO_FABRICACION_4', 'TIEMPO_FABRICACION_5', 'TIEMPO_FABRICACION_6 - 10', 'TIEMPO_FABRICACION_>10', 'TIEMPO_FABRICACION_nan', 'tiempo', 'FECHA_INICIO', 'FECHA_FIN']\n"
     ]
    }
   ],
   "source": [
    "print list(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified that the feature `tiempo` is displayed in days. We add three more features that could possibly help us understand time better:\n",
    "\n",
    "1. `tiempo_anos`: years the client has been a client\n",
    "2. `tiempo_sem`: semesters the client has been a client\n",
    "3. `tiempo_trim`: quarters the client has been a client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['tiempo_anos'] = ((data['tiempo'] / 365).astype('int') + 1)\n",
    "data['tiempo_sem'] = ((data['tiempo'] / 182.5).astype('int') + 1)\n",
    "data['tiempo_trim'] = ((data['tiempo'] / 91.25).astype('int') + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove some features we don't need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_data_1 = data[['tiempo','ABANDONO_A',\n",
    "    'LIMA_PROVINCIAS_CONO', 'LIMA_PROVINCIAS_LIMA TRADICIONAL', 'LIMA_PROVINCIAS_PROVINCIA',\n",
    "    'NSE_A', 'NSE_A1', 'NSE_A2', 'NSE_B1', 'NSE_B2', 'NSE_C1', 'NSE_C2', 'NSE_D1', 'NSE_D2', 'NSE_E',\n",
    "    'SEGMENTO_MASIVO', 'SEGMENTO_ORO 1', 'SEGMENTO_ORO 2', 'SEGMENTO_ORO 3', 'SEGMENTO_PLATINO 1', 'SEGMENTO_PLATINO 2',\n",
    "    'n_EDAD',\n",
    "    'MARCA_AUDI', 'MARCA_BAJAJ', 'MARCA_BMW', 'MARCA_CHERY', 'MARCA_CHEVROLET', 'MARCA_CITROEN', 'MARCA_DAIHATSU', \n",
    "    'MARCA_DODGE', 'MARCA_FORD', 'MARCA_GREAT WALL', 'MARCA_HONDA', 'MARCA_HYUNDAI', 'MARCA_JAC', 'MARCA_JEEP', 'MARCA_KIA', \n",
    "    'MARCA_MAZDA', 'MARCA_MERCEDES', 'MARCA_MITSUBISHI', 'MARCA_NISSAN', 'MARCA_OTROS', 'MARCA_RENAULT', 'MARCA_SEAT', \n",
    "    'MARCA_SSANGYONG', 'MARCA_SUBARU', 'MARCA_SUZUKI', 'MARCA_TOYOTA', 'MARCA_VOLKSWAGEN', 'MARCA_VOLVO',\n",
    "    'USO_VEHICULO_INICIAL_CARGA', 'USO_VEHICULO_INICIAL_COMERCIAL', 'USO_VEHICULO_INICIAL_OTRO', \n",
    "    'USO_VEHICULO_INICIAL_PARTICULAR', 'USO_VEHICULO_INICIAL_PUBLICO', 'USO_VEHICULO_INICIAL_TAXI URBANO', \n",
    "    'USO_VEHICULO_INICIAL_TRANSPORTE DE PERSONAL', 'USO_VEHICULO_INICIAL_VEHICULO DE ALQUILER',\n",
    "    'n_TIEMPO_FABRICACION',\n",
    "    'PRIMA_INICIAL',\n",
    "    'CANAL_CORREDOR', 'CANAL_DIRECTO', 'CANAL_FUERZA DE VENTAS', 'CANAL_NO TRADICIONAL',\n",
    "    'CORREDOR_MARSH', 'CORREDOR_CONSEJEROS_CORREDORES_SEGUROS', 'CORREDOR_CORREDORES_SEGUROS_FALABELLA', \n",
    "    'CORREDOR_PRIMERA_CORREDORES_SEGURO', 'CORREDOR_MARIATEGUI', 'CORREDOR_LA_UNION', 'CORREDOR_CARLOS_UGAZ', \n",
    "    'CORREDOR_GERENCIA_RIESGOS', 'CORREDOR_GABEL', 'CORREDOR_CONTACTO_CORREDORES', 'CORREDOR_MN_ASOC', \n",
    "    'CORREDOR_AMERICA_BROKERS', 'CORREDOR_FBA_CORREDORES', 'CORREDOR_OTROS',\n",
    "    'MONTO_SINIESTRO_SUMA', \n",
    "    'RECLAMO', 'DISCONFORMIDADES',\n",
    "    'n_NUMERO_SINIESTROS_FINAL',\n",
    "    'n_NUM_RIESGOS',\n",
    "    'n_NUM_PRODUCTOS',\n",
    "    'n_NUMERO_SINIESTROS_TOTAL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERDIDA_INICIAL       59355\n",
      "DUDOSO_INICIAL        59355\n",
      "DEFICIENTE_INICIAL    59355\n",
      "CPP_INICIAL           59355\n",
      "OK_INICIAL            59355\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "df1_nulls = churn_data_1.apply(num_missing, axis=0)\n",
    "print df1_nulls[df1_nulls > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_data_1 = data.drop([\n",
    "        'Unnamed: 0',\n",
    "        'CAMBIO_CARRO',\n",
    "        'SUMA_ASEGURADA_MEDIANA',\n",
    "        'SUMA_ASEGURADA_SUMA',\n",
    "        'PRIMA_TOTAL',\n",
    "        'MONTO_SINIESTRO_MEDIANA',\n",
    "        'MONTO_SINIESTRO_SUMA',\n",
    "        'ANO_FABRICACION_2000-2007',\n",
    "        'ANO_FABRICACION_2008',\n",
    "        'ANO_FABRICACION_2009',\n",
    "        'ANO_FABRICACION_2010',\n",
    "        'ANO_FABRICACION_2011',\n",
    "        'ANO_FABRICACION_2012',\n",
    "        'ANO_FABRICACION_2013',\n",
    "        'ANO_FABRICACION_2014',\n",
    "        'ANO_FABRICACION_2015',\n",
    "        'ANO_FABRICACION_2016',\n",
    "        'ANO_FABRICACION_>2000',\n",
    "        'ANO_FABRICACION_nan',\n",
    "        'COLOR_nan',\n",
    "        'MARCA_nan',\n",
    "        'MODELO_nan',\n",
    "        'TIPO_VEHICULO_nan',\n",
    "        'EDAD_nan',\n",
    "        'TIPO_VEHICULO_VAN',\n",
    "        'LIMA_PROVINCIAS_nan',\n",
    "        'NSE_nan',\n",
    "        'SEGMENTO_MASIVO',\n",
    "        'SEGMENTO_ORO 1',\n",
    "        'SEGMENTO_ORO 2',\n",
    "        'SEGMENTO_ORO 3',\n",
    "        'SEGMENTO_PLATINO 1',\n",
    "        'SEGMENTO_PLATINO 2',\n",
    "        'SEGMENTO_nan',\n",
    "        'TIPO_PLAN_nan',\n",
    "        'PLAN_BASICO',\n",
    "        'PLAN_COLECTIVO',\n",
    "        'PLAN_CORPORATIVO',\n",
    "        'PLAN_MODULAR',\n",
    "        'PLAN_NO_TRADICIONAL',\n",
    "        'PLAN_PREMIER',\n",
    "        'PLAN_PROVINCIA',\n",
    "        'TIEMPO_FABRICACION_nan',\n",
    "        'NUMERO_SINIESTROS_TOTAL_0',\n",
    "        'NUMERO_SINIESTROS_TOTAL_1',\n",
    "        'NUMERO_SINIESTROS_TOTAL_2',\n",
    "        'NUMERO_SINIESTROS_TOTAL_3 - 4',\n",
    "        'NUMERO_SINIESTROS_TOTAL_> 5',\n",
    "        'NUMERO_CERTIFICADOS_1',\n",
    "        'NUMERO_CERTIFICADOS_2',\n",
    "        'NUMERO_CERTIFICADOS_3',\n",
    "        'NUMERO_CERTIFICADOS_4',\n",
    "        'NUMERO_CERTIFICADOS_>4',\n",
    "        'NUMERO_SINIESTROS_FINAL_0',\n",
    "        'NUMERO_SINIESTROS_FINAL_1',\n",
    "        'NUMERO_SINIESTROS_FINAL_2',\n",
    "        'NUMERO_SINIESTROS_FINAL_3 - 4',\n",
    "        'NUMERO_SINIESTROS_FINAL_> 5',\n",
    "        'NUM_RIESGOS_0',\n",
    "        'NUM_RIESGOS_1',\n",
    "        'NUM_RIESGOS_2',\n",
    "        'NUM_RIESGOS_>3',\n",
    "        'NUM_PRODUCTOS_0', \n",
    "        'NUM_PRODUCTOS_1',\n",
    "        'NUM_PRODUCTOS_2',\n",
    "        'NUM_PRODUCTOS_3',\n",
    "        'NUM_PRODUCTOS_>4',\n",
    "        'EDAD_30-35',\n",
    "        'EDAD_35-40',\n",
    "        'EDAD_40-50', \n",
    "        'EDAD_50-60',\n",
    "        'EDAD_<30',\n",
    "        'EDAD_>60',\n",
    "        'TIEMPO_FABRICACION_0',\n",
    "        'TIEMPO_FABRICACION_1',\n",
    "        'TIEMPO_FABRICACION_2',\n",
    "        'TIEMPO_FABRICACION_3',\n",
    "        'TIEMPO_FABRICACION_4',\n",
    "        'TIEMPO_FABRICACION_5',\n",
    "        'TIEMPO_FABRICACION_6 - 10',\n",
    "        'TIEMPO_FABRICACION_>10',\n",
    "        'tiempo_anos',\n",
    "        'tiempo_sem',\n",
    "        'tiempo_trim',\n",
    "        'ABANDONO_NA',\n",
    "        'ANULADO_NA', \n",
    "        'NO_RENOVACION_NA',\n",
    "        'n_ANO_FABRICACION',\n",
    "        'n_NUMERO_CERTIFICADOS',\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separate our features and target columns as well as our labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['tiempo', 'LIMA_PROVINCIAS_CONO', 'LIMA_PROVINCIAS_LIMA TRADICIONAL', 'LIMA_PROVINCIAS_PROVINCIA', 'NSE_A', 'NSE_A1', 'NSE_A2', 'NSE_B1', 'NSE_B2', 'NSE_C1', 'NSE_C2', 'NSE_D1', 'NSE_D2', 'NSE_E', 'SEGMENTO_MASIVO', 'SEGMENTO_ORO 1', 'SEGMENTO_ORO 2', 'SEGMENTO_ORO 3', 'SEGMENTO_PLATINO 1', 'SEGMENTO_PLATINO 2', 'n_EDAD', 'MARCA_AUDI', 'MARCA_BAJAJ', 'MARCA_BMW', 'MARCA_CHERY', 'MARCA_CHEVROLET', 'MARCA_CITROEN', 'MARCA_DAIHATSU', 'MARCA_DODGE', 'MARCA_FORD', 'MARCA_GREAT WALL', 'MARCA_HONDA', 'MARCA_HYUNDAI', 'MARCA_JAC', 'MARCA_JEEP', 'MARCA_KIA', 'MARCA_MAZDA', 'MARCA_MERCEDES', 'MARCA_MITSUBISHI', 'MARCA_NISSAN', 'MARCA_OTROS', 'MARCA_RENAULT', 'MARCA_SEAT', 'MARCA_SSANGYONG', 'MARCA_SUBARU', 'MARCA_SUZUKI', 'MARCA_TOYOTA', 'MARCA_VOLKSWAGEN', 'MARCA_VOLVO', 'USO_VEHICULO_INICIAL_CARGA', 'USO_VEHICULO_INICIAL_COMERCIAL', 'USO_VEHICULO_INICIAL_OTRO', 'USO_VEHICULO_INICIAL_PARTICULAR', 'USO_VEHICULO_INICIAL_PUBLICO', 'USO_VEHICULO_INICIAL_TAXI URBANO', 'USO_VEHICULO_INICIAL_TRANSPORTE DE PERSONAL', 'USO_VEHICULO_INICIAL_VEHICULO DE ALQUILER', 'n_TIEMPO_FABRICACION', 'PRIMA_INICIAL', 'CANAL_CORREDOR', 'CANAL_DIRECTO', 'CANAL_FUERZA DE VENTAS', 'CANAL_NO TRADICIONAL', 'CORREDOR_MARSH', 'CORREDOR_CONSEJEROS_CORREDORES_SEGUROS', 'CORREDOR_CORREDORES_SEGUROS_FALABELLA', 'CORREDOR_PRIMERA_CORREDORES_SEGURO', 'CORREDOR_MARIATEGUI', 'CORREDOR_LA_UNION', 'CORREDOR_CARLOS_UGAZ', 'CORREDOR_GERENCIA_RIESGOS', 'CORREDOR_GABEL', 'CORREDOR_CONTACTO_CORREDORES', 'CORREDOR_MN_ASOC', 'CORREDOR_AMERICA_BROKERS', 'CORREDOR_FBA_CORREDORES', 'CORREDOR_OTROS', 'MONTO_SINIESTRO_SUMA', 'RECLAMO', 'DISCONFORMIDADES', 'n_NUMERO_SINIESTROS_FINAL', 'n_NUM_RIESGOS', 'n_NUM_PRODUCTOS', 'n_NUMERO_SINIESTROS_TOTAL']\n",
      "\n",
      "Target: ['ABANDONO_A']\n"
     ]
    }
   ],
   "source": [
    "churn_labels = churn_data_1.columns.tolist()\n",
    "feature_cols = churn_data_1.drop(['ABANDONO_A'], axis=1).columns.tolist()\n",
    "target_col = ['ABANDONO_A']\n",
    "class_names = ['STAY', 'CHURN']\n",
    "\n",
    "print \"Features: {}\".format(feature_cols)\n",
    "print \"\\nTarget: {}\".format(target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and cross-validation\n",
    "\n",
    "We need to make sure our data is given the same importance across features. To do this we normalize our values using the [`scale()`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) function provided by the [`preprocessing`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) package to create the `churn_norm` data frame. Then we slice the features we need for `X_all` and the target feature for `y_all`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_norm = pandas.DataFrame(preprocessing.scale(churn_data_1.values), columns=churn_labels)\n",
    "\n",
    "X_all = churn_data_1[feature_cols]\n",
    "y_all = churn_data_1[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our data into training and testing sets. The models first train using some part of the data (training set) and then once some rules for classification have been established, test using another part of the data (testing set). \n",
    "\n",
    "The cross validation function we are using is called [`ShuffleSplit()`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.ShuffleSplit.html#sklearn.cross_validation.ShuffleSplit) from the [`cross_validation`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation) package. This method performs a random permutation cross-validation iteratively for a specified number of iterations and partition size. We are using 100 iterations and a 30-70 test-training partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 163258 samples.\n",
      "The testing set has 69969 samples.\n"
     ]
    }
   ],
   "source": [
    "X = numpy.array(X_all)\n",
    "y = numpy.array(y_all)\n",
    "\n",
    "for i, j in ShuffleSplit(len(X), n_iter=100, test_size=.3):\n",
    "    X_train, X_test = X[i], X[j]\n",
    "    y_train, y_test = y[i], y[j]\n",
    "\n",
    "print \"The training set has {} samples.\".format(X_train.shape[0])\n",
    "print \"The testing set has {} samples.\".format(X_test.shape[0])\n",
    "\n",
    "X_train, X_test = pandas.DataFrame(X_train), pandas.DataFrame(X_test)\n",
    "y_train, y_test = pandas.DataFrame(y_train), pandas.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing different models\n",
    "Now that the data is ready and cross-validated, we can start to compare different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\n",
      "Training set accuracy: 99.995%\n",
      "Test set accuracy: 85.255%\n",
      "\n",
      "Training set F1 score: 99.995%\n",
      "Test set F1 score: 83.582%\n",
      "\n",
      "GaussianNB()\n",
      "\n",
      "Training set accuracy: 68.676%\n",
      "Test set accuracy: 68.167%\n",
      "\n",
      "Training set F1 score: 62.523%\n",
      "Test set F1 score: 61.872%\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Training set accuracy: 99.147%\n",
      "Test set accuracy: 86.388%\n",
      "\n",
      "Training set F1 score: 99.042%\n",
      "Test set F1 score: 83.781%\n",
      "\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "Training set accuracy: 99.995%\n",
      "Test set accuracy: 81.911%\n",
      "\n",
      "Training set F1 score: 99.995%\n",
      "Test set F1 score: 78.060%\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "\n",
      "Training set accuracy: 84.068%\n",
      "Test set accuracy: 77.900%\n",
      "\n",
      "Training set F1 score: 82.150%\n",
      "Test set F1 score: 75.304%\n",
      "\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "\n",
      "Training set accuracy: 85.927%\n",
      "Test set accuracy: 85.638%\n",
      "\n",
      "Training set F1 score: 82.946%\n",
      "Test set F1 score: 82.567%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "# SVC = SVC()\n",
    "GNB = GaussianNB()\n",
    "RFC = RandomForestClassifier()\n",
    "ETC = ExtraTreesClassifier()\n",
    "KNN = KNeighborsClassifier()\n",
    "GBC = GradientBoostingClassifier()\n",
    "\n",
    "for classifier in [DTC, GNB, RFC, ETC, KNN, GBC]:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print classifier\n",
    "    print '\\nTraining set accuracy: {:.3f}%'.format(accuracy_score(y_train.values, classifier.predict(X_train))*100)\n",
    "    print 'Test set accuracy: {:.3f}%'.format(accuracy_score(y_test.values, classifier.predict(X_test))*100)\n",
    "    print '\\nTraining set F1 score: {:.3f}%'.format(f1_score(y_train.values, classifier.predict(X_train))*100)\n",
    "    print 'Test set F1 score: {:.3f}%\\n'.format(f1_score(y_test.values, classifier.predict(X_test))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model\n",
    "After looking at the results, we have determined the **Decision Tree Classifier** to more accurately classify clients that have churned.\n",
    "\n",
    "First, we instance the classifier with a Gini impurity criterion. This criterion affects the way the Decision Tree branches out. Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It can be computed by summing the probability $f_i$ of an item with label $i$ being chosen times the probability $1-f_{i}$ of a mistake in categorizing that item. It reaches its minimum (0) when all cases in the node fall into a single target category. This criterion is useful for this kind of data because it deals better with continuous values than information gain or entropy.\n",
    "\n",
    "Next, we utilize a maximum depth of 8 meaning the Decision Tree will reach 8 branch levels downwards at the most. This parameter is very important for this model, given the large amount of features and observations, it allows to reduce the specificity of the rules we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(criterion='gini', max_depth=8).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We score our model's accuracy and F1-measure both on the test set and the training set. Testing the model over the test set allows us to see how well our model performs on **different data** than the one that was used for training. Testing over the training set allows us to see how well we are performing over the **same data** that we used to train it.\n",
    "\n",
    "The value of testing over both sets is to see **how different our results are**. This can give us some insight over how representative was the sampling method used, and if the model underfits or overfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 82.204%\n",
      "Training set accuracy: 82.574%\n",
      "Accuracy differential: 0.371pp\n",
      "\n",
      "Test set F1 score: 78.684%\n",
      "Training set F1 score: 79.140%\n",
      "F1 score differential: 0.456pp\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = accuracy_score(y_test, DTC.predict(X_test)) * 100\n",
    "accuracy_training = accuracy_score(y_train, DTC.predict(X_train)) * 100\n",
    "print 'Test set accuracy: {:.3f}%'.format(accuracy_test)\n",
    "print 'Training set accuracy: {:.3f}%'.format(accuracy_training)\n",
    "print 'Accuracy differential: {:.3f}pp'.format(abs(accuracy_test - accuracy_training))\n",
    "\n",
    "f1_test = f1_score(y_test, DTC.predict(X_test)) * 100\n",
    "f1_training = f1_score(y_train, DTC.predict(X_train)) * 100\n",
    "print '\\nTest set F1 score: {:.3f}%'.format(f1_test)\n",
    "print 'Training set F1 score: {:.3f}%'.format(f1_training)\n",
    "print 'F1 score differential: {:.3f}pp'.format(abs(f1_test - f1_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon agreeing on the similarities between the training and test sets, we set to evaluate the relationship between precision and recall through a confusion matrix. In this graph, we can tell how many true/false positives and true/false negatives we have obtained through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "cm = confusion_matrix(y_test, DTC.predict(X_test))\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "\n",
    "precision = float(cm[0][0]) / (cm[0][0] + cm[0][1]) * 100\n",
    "recall = float(cm[0][0]) / (cm[0][0] + cm[1][0]) * 100\n",
    "\n",
    "numpy.set_printoptions(precision=2)\n",
    "pyplot.figure()\n",
    "pyplot.imshow(cm, interpolation='nearest', cmap=pyplot.cm.Blues)\n",
    "pyplot.title('Confusion matrix')\n",
    "pyplot.colorbar()\n",
    "tick_marks = numpy.arange(len(class_names))\n",
    "pyplot.xticks(tick_marks, class_names, rotation=45)\n",
    "pyplot.yticks(tick_marks, class_names)\n",
    "pyplot.tight_layout()\n",
    "pyplot.ylabel('TRUE')\n",
    "pyplot.xlabel('PREDICTED')\n",
    "cm = pandas.DataFrame(cm)\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "print cm\n",
    "print '\\nPrecision: {:.2f}%'.format(precision)\n",
    "print 'Recall: {:.2f}%'.format(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results tell us that we can predict equally as well for clients that will stay and those that will churn.\n",
    "\n",
    "We can now output our model to a PDF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(DTC, out_file=dot_data, feature_names=feature_cols, class_names=class_names, filled=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf(\"model.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the top 10 most important features and their contribution towards reducing uncertainty through Gini impurity reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_features = 10\n",
    "importances = DTC.feature_importances_\n",
    "std = numpy.std(DTC.feature_importances_,axis=0)\n",
    "indices = numpy.argsort(importances)[::-1]\n",
    "\n",
    "print '\\nTop 10 features based on Gini impurity:'\n",
    "for i in range(number_of_features):\n",
    "    print '{}. {} (#{}): {:.3f}'.format(i + 1, feature_cols[i-1], indices[i], importances[indices[i]])\n",
    "\n",
    "pyplot.figure()\n",
    "pyplot.title(\"Feature importances\")\n",
    "pyplot.bar(range(number_of_features), importances[indices[:number_of_features]], color=\"gray\", align=\"center\")\n",
    "pyplot.xticks(range(number_of_features), indices)\n",
    "pyplot.xlim([-1, number_of_features])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rules = data[\n",
    "    ((data['NSE_E'] == 1) | (data['NSE_nan'] == 1))\n",
    "    & (data['CAMBIO_PRIMA_UN PERIODO'] == 1)\n",
    "    & (data['SUMA_ASEGURADA_MEDIA'] < (data['SUMA_ASEGURADA_MEDIA'].mean()+10))\n",
    "    & (data['CANAL_NO_TRADICIONAL'] == 1)\n",
    "    & (data['CORREDOR_MARSH'] == 0)\n",
    "    & (data['CORREDOR_AMERICA_BROKERS'] == 0)\n",
    "    & (data['CORREDOR_CONSEJEROS_CORREDORES_SEGUROS'] == 0)\n",
    "    & (data['CORREDOR_CORREDORES_SEGUROS_FALABELLA'] == 0)\n",
    "    & ((data['CORREDOR_MARIATEGUI'] == 1) | (data['CORREDOR_LA_UNION'] == 1))\n",
    "    & (data['PRIMA_FINAL'] < (data['PRIMA_FINAL'].mean()-72))\n",
    "]\n",
    "\n",
    "rule_matches = len(rules)\n",
    "churn_matches = len(rules.loc[rules['ABANDONO_A'] == 1])\n",
    "\n",
    "if not(rules.empty):\n",
    "    print 'Clients found by rule: {}'.format(rule_matches)\n",
    "    print 'Clients that churned: {}'.format(churn_matches)\n",
    "    print 'Rule accuracy: {:.3f}%'.format(churn_matches / float(rule_matches) * 100)\n",
    "else:\n",
    "    print 'No rules found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NSE = [\n",
    "    'NSE_A',\n",
    "    'NSE_A1',\n",
    "    'NSE_A2',\n",
    "    'NSE_B1',\n",
    "    'NSE_B2',\n",
    "    'NSE_C1',\n",
    "    'NSE_C2',\n",
    "    'NSE_D1',\n",
    "    'NSE_D2',\n",
    "    'NSE_E',\n",
    "    'NSE_nan',\n",
    "]\n",
    "\n",
    "corredores = [\n",
    "    'CORREDOR_MARSH', \n",
    "    'CORREDOR_CONSEJEROS_CORREDORES_SEGUROS', \n",
    "    'CORREDOR_CORREDORES_SEGUROS_FALABELLA',\n",
    "    'CORREDOR_PRIMERA_CORREDORES_SEGURO', \n",
    "    'CORREDOR_MARIATEGUI', \n",
    "    'CORREDOR_LA_UNION', \n",
    "    'CORREDOR_CARLOS_UGAZ', \n",
    "    'CORREDOR_GERENCIA_RIESGOS', \n",
    "    'CORREDOR_GABEL', \n",
    "    'CORREDOR_CONTACTO_CORREDORES', \n",
    "    'CORREDOR_MN_ASOC', \n",
    "    'CORREDOR_AMERICA_BROKERS', \n",
    "    'CORREDOR_FBA_CORREDORES', \n",
    "    'CORREDOR_OTROS'\n",
    "]\n",
    "\n",
    "canales = [\n",
    "    'CANAL_CORREDOR', \n",
    "    'CANAL_NO_TRADICIONAL', \n",
    "    'CANAL_DIRECTO', \n",
    "    'CANAL_FUERZA_VENTAS',\n",
    "    'CANAL_NO_DETERMINADO'\n",
    "]\n",
    "\n",
    "riesgos = [\n",
    "    'RIESGOEPSYSCTRSALUD', \n",
    "    'RIESGORIESGOSGENERALES', \n",
    "    'RIESGOSALUD', \n",
    "    'RIESGOVEHICULOSYSOAT', \n",
    "    'RIESGOVIDA'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pattern(pattern):\n",
    "    for i in range(len(pattern)):\n",
    "        if (i == 0):\n",
    "            rules = (data[pattern[i]] == 1)\n",
    "        else:\n",
    "            if not(i+1 == len(pattern)):\n",
    "                rules = rules | (data[pattern[i+1]] == 1)\n",
    "\n",
    "    rule_matches = pandas.DataFrame(data[rules])\n",
    "    match_count = len(rule_matches)\n",
    "    churn_matches = len(rule_matches.loc[rule_matches['ABANDONO_A'] == 1])\n",
    "\n",
    "    if not(rule_matches.empty):\n",
    "        print 'Clients found by rule: {}'.format(match_count)\n",
    "        print 'Clients that churned: {}'.format(churn_matches)\n",
    "        print 'Rule accuracy: {:.3f}%'.format(churn_matches / float(match_count) * 100)\n",
    "    else:\n",
    "        print 'No rules found'\n",
    "\n",
    "    baseline = churn_matches / float(match_count)\n",
    "\n",
    "    for i in range(len(pattern)):\n",
    "        rules = data[(data[pattern[i]] == 1)]\n",
    "        rule_matches = len(rules)\n",
    "        churn_matches = len(rules.loc[rules['ABANDONO_A'] == 1])\n",
    "\n",
    "        if not(rules.empty):\n",
    "            print '\\n' + pattern[i]\n",
    "            print 'Clients found by rule: {}'.format(rule_matches)\n",
    "            print 'Clients that churned: {}'.format(churn_matches)\n",
    "            print 'Rule accuracy: {:.3f}%'.format(churn_matches / float(rule_matches) * 100)\n",
    "            print 'Churn rate change: {:.3f}pp'.format((churn_matches / float(rule_matches) - baseline) * 100)\n",
    "        else:\n",
    "            print 'No rules found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pattern(riesgos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(corredores)):\n",
    "    if (i == 0):\n",
    "        rules = (data[corredores[i]] == 1)\n",
    "    else:\n",
    "        if not(i+1 == len(corredores)):\n",
    "            rules = rules | (data[corredores[i+1]] == 1)\n",
    "            \n",
    "rule_matches = pandas.DataFrame(data[rules])\n",
    "match_count = len(rule_matches)\n",
    "churn_matches = len(rule_matches.loc[rule_matches['ABANDONO_A'] == 1])\n",
    "\n",
    "if not(rule_matches.empty):\n",
    "    print 'Clients found by rule: {}'.format(match_count)\n",
    "    print 'Clients that churned: {}'.format(churn_matches)\n",
    "    print 'Rule accuracy: {:.3f}%'.format(churn_matches / float(match_count) * 100)\n",
    "else:\n",
    "    print 'No rules found'\n",
    "    \n",
    "baseline = churn_matches / float(match_count)\n",
    "\n",
    "for i in range(len(corredores)):\n",
    "    rules = data[(data[corredores[i]] == 1)]\n",
    "    rule_matches = len(rules)\n",
    "    churn_matches = len(rules.loc[rules['ABANDONO_A'] == 1])\n",
    "\n",
    "    if not(rules.empty):\n",
    "        print '\\n' + corredores[i]\n",
    "        print 'Clients found by rule: {}'.format(rule_matches)\n",
    "        print 'Clients that churned: {}'.format(churn_matches)\n",
    "        print 'Rule accuracy: {:.3f}%'.format(churn_matches / float(rule_matches) * 100)\n",
    "        print 'Churn rate change: {:.3f}pp'.format((churn_matches / float(rule_matches) - baseline) * 100)\n",
    "    else:\n",
    "        print 'No rules found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data[(data['CORREDOR_CARLOS_UGAZ'] == 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(churn_data_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "from lifelines import KaplanMeierFitter\n",
    "T = churn_data_1['tiempo'][churn_data_1['tiempo']<2000]\n",
    "E = churn_data_1['ABANDONO_A'][churn_data_1['tiempo']<2000]\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(T, event_observed=E) # more succiently, kmf.fit(T,E)\n",
    "\n",
    "p = kmf.plot(ci_force_lines=True, title='Survival Analysis')\n",
    "\n",
    "# Collect the plot object\n",
    "kmf = plt.gcf() \n",
    "\n",
    "def pyplot(fig, ci=True, legend=True):\n",
    "    # Convert mpl fig obj to plotly fig obj, resize to plotly's default\n",
    "    py_fig = tls.mpl_to_plotly(fig, resize=True)\n",
    "    \n",
    "    # Add fill property to lower limit line\n",
    "    if ci == True:\n",
    "        style1 = dict(fill='tonexty')\n",
    "        # apply style\n",
    "        py_fig['data'][2].update(style1)\n",
    "        \n",
    "        # Change color scheme to black\n",
    "        py_fig['data'].update(dict(line=Line(color='blue')))\n",
    "    \n",
    "    # change the default line type to 'step'\n",
    "    py_fig['data'].update(dict(line=Line(shape='hv')))\n",
    "    # Delete misplaced legend annotations \n",
    "    py_fig['layout'].pop('annotations', None)\n",
    "    \n",
    "    if legend == True:\n",
    "        # Add legend, place it at the top right corner of the plot\n",
    "        py_fig['layout'].update(\n",
    "            showlegend=True,\n",
    "            legend=Legend(\n",
    "                x=1.05,\n",
    "                y=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Send updated figure object to Plotly, show result in notebook\n",
    "    return py.iplot(py_fig)\n",
    "\n",
    "pyplot(kmf, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "data_1 = churn_data_1[['tiempo','ABANDONO_A','NSE_A', 'NSE_A1', 'NSE_A2', 'NSE_B1', 'NSE_B2', 'NSE_C1', 'NSE_C2', 'NSE_D1', \\\n",
    "'NSE_D2', 'NSE_E','n_TIEMPO_FABRICACION','n_EDAD','CANAL_CORREDOR', 'CANAL_NO_TRADICIONAL', 'CANAL_DIRECTO', \\\n",
    "'n_NUMERO_SINIESTROS_TOTAL', 'n_NUMERO_SINIESTROS_FINAL', 'n_NUM_RIESGOS', 'n_NUM_PRODUCTOS',\\\n",
    "'TIPO_VEHICULO_AUTO', 'TIPO_VEHICULO_MOTO', 'TIPO_VEHICULO_RURAL','CORREDOR_MARSH', \\\n",
    "'CORREDOR_CONSEJEROS_CORREDORES_SEGUROS', 'CORREDOR_CORREDORES_SEGUROS_FALABELLA', 'CORREDOR_PRIMERA_CORREDORES_SEGURO', \\\n",
    "'CORREDOR_MARIATEGUI', 'CORREDOR_LA_UNION', 'CORREDOR_CARLOS_UGAZ', \\\n",
    "'CORREDOR_AMERICA_BROKERS', 'CORREDOR_FBA_CORREDORES', 'CORREDOR_OTROS']]\n",
    "\n",
    "cf = CoxPHFitter()\n",
    "cf.fit(data_1, 'tiempo', event_col= 'ABANDONO_A')\n",
    "\n",
    "cf.print_summary()  # access the results using cf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_data_2 = churn_data_1[churn_data_1['tiempo']<2000]\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "T = churn_data_2['tiempo']\n",
    "C = churn_data_2['ABANDONO_A']\n",
    "kmf.fit(T, event_observed=C, label=['BASELINE'])\n",
    "kmf.survival_function_.plot(ax=ax)\n",
    "\n",
    "lista_col = ['CORREDOR_MARSH', 'CORREDOR_CONSEJEROS_CORREDORES_SEGUROS', 'CORREDOR_CORREDORES_SEGUROS_FALABELLA', \\\n",
    "             'CORREDOR_PRIMERA_CORREDORES_SEGURO', 'CORREDOR_MARIATEGUI', 'CORREDOR_LA_UNION', 'CORREDOR_CARLOS_UGAZ', \\\n",
    "'CORREDOR_AMERICA_BROKERS', 'CORREDOR_FBA_CORREDORES', 'CORREDOR_OTROS', 'CORREDOR_GABEL']\n",
    "\n",
    "for col in lista_col:\n",
    "    print col + ': ' + str(len(churn_data_2[churn_data_2[col]==1])) + ' (churn: ' \\\n",
    "    + str(len(churn_data_2[(churn_data_2[col]==1) & (churn_data_2['ABANDONO_A']==1)])) + ')'\n",
    "    f2 = churn_data_2[col]==1\n",
    "    T2 = churn_data_2[f2]['tiempo']\n",
    "    C2 = churn_data_2[f2]['ABANDONO_A']\n",
    "    kmf.fit(T2, event_observed=C2, label=[col])\n",
    "    kmf.survival_function_.plot(ax=ax)\n",
    "\n",
    "plt.title('Survival Analysis')\n",
    "kmf2 = plt.gcf()\n",
    "pyplot(kmf2, ci=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
